## LLMs: Theory and Practice

This page aims at providing usefull papers to understand the theory beneath the Large Language Models, as well as tools to leverage these models in real-world use cases.

---

### Papers

[Alacapa: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)

[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)

[Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)

[What Learning Algorithm is In-Context Learning? Investigations with Linear Models](https://arxiv.org/pdf/2211.15661.pdf)

[What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/pdf/2204.05832.pdf)

---

### Articles

[In-context learning (MIT)](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)

[Prompt engineering (cohere)](https://docs.cohere.ai/docs/prompt-engineering)

[Prompt engineering (The Gradient)](https://thegradient.pub/prompting/)

[Stanford Alpaca, and the acceleration of on-device large language model development](https://simonwillison.net/2023/Mar/13/alpaca/)

[LangChain and LlamaIndex book](https://leanpub.com/langchain/read)

---

### Tools

[LangChain](https://python.langchain.com/)

[React](https://til.simonwillison.net/llms/python-react-pattern)

---

### Pretrained models

[GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)

[GPT4All](https://github.com/nomic-ai/gpt4all)

[BigScience (BLOOM and variants)](https://huggingface.co/bigscience)

[SGPT](https://github.com/Muennighoff/sgpt#use-sgpt-with-sentence-transformers)

