## LLMs: Theory and Practice

This page aims at providing usefull papers to understand the theory beneath the Large Language Models, as well as tools to leverage these models in real-world use cases.

---

### Papers

[Language Models are Unsupervised Multitask Learners](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf)

[WHAT LEARNING ALGORITHM IS IN-CONTEXT LEARNING? INVESTIGATIONS WITH LINEAR MODELS](https://arxiv.org/pdf/2211.15661.pdf)

[What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/pdf/2204.05832.pdf)

---

### Articles

[In-context learning (MIT)](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)

[Prompt engineering (cohere)](https://docs.cohere.ai/docs/prompt-engineering)

[Prompt engineering (The Gradient)](https://thegradient.pub/prompting/)

---

### Tools

[LangChain](https://python.langchain.com/)

[React](https://til.simonwillison.net/llms/python-react-pattern)

---

### Pretrained models

[BigScience (BLOOM and variants)](https://huggingface.co/bigscience)

[SGPT](https://github.com/Muennighoff/sgpt#use-sgpt-with-sentence-transformers)

